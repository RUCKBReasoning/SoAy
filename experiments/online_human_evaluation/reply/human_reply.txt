Geoffrey E. Hinton，以及Martín Abadi、Michael Burrows
1. A Survey on Heterogeneous Graph Embedding\n2. Graph Self-Supervised Learning: A Survey\n3. Meta-Learning with GNs:Methods&Applications\n4. Automated Machine Learning on Graphs: A Survey
主页：http://nlp.csai.tsinghua.edu.cn/~lzy/\n邮箱：liuzy@tsinghua.edu.cn
1、《Task-Agnostic and Adaptive-Size BERT Compression》；《Data-Aware Low-Rank Compression for Large NLP Models》；《Speeding up Deep Learning Training by Sharing Weights and then Unsharing》；《Dact-BERT: Increasing the Efficiency and Interpretability of BERT by Using Adaptive Computation Time》
组织ICDM PAKDD workshop，担任KDD、AAAI、IJCAI、IEEE Transaction on Cybernetics等多个期刊和会议的审稿人，并在多个国际会议担任Track Chair和程序委员会委员
谢涛、刘铁岩、李飞飞
不是
Sep-18
大牛学者
A. V. Tsiganov
无合适回答
谷歌开发者
ChatGPT: Jack of all trades, master of none；NLPLego: Assembling Test Generation for Natural Language Processing Applications；Enabling Human-like Language-Capable Robots Through Working Memory Modeling；More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking；APIContext2Com: Code Comment Generation by Incorporating Pre-Defined API Documentation
Synthesizing Mixed-type Electronic Health Records using Diffusion Models；EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models；Exploring contrast generalisation in deep learning-based brain MRI-to-CT synthesis；Decision-BADGE: Decision-based Adversarial Batch Attack with Directional Gradient Estimation；Implementation of Generative Adversarial Networks in Mobile Applications for Image Data Enhancement
yann@cs.nyu.edu&yann@fb.com
Toolformer 是一个大型语言模型，通过使用 In-Context Learning 来提高模型理解和生成适合给定上下文或情况的语言能力。 它使用 API 调用来注释大量数据，然后使用这些 API 调用对模型进行微调，以进行有用的 API 调用。
是，2023年有发表论文Performance-aware Approximation of Global Channel Pruning for Multitask CNNs
Yee Whye Teh，Ting Chen，Li Deng，Dong Yu(俞栋)，Boyang Deng
David B. Dunson，from Duke Institute for Brain Sciences
CVPR: IEEE/CVF Conference on Computer Vision and Pattern Recognition，NeurIPS :Neural Information Processing Systems (NIPS)，ICCV: IEEE/CVF International Conference on Computer Vision，ECCV: European Conference on Computer Vision，AAAI: AAAI Conference on Artificial Intelligence
Wang Jingdong,\nSun Ke,\nCheng Tianheng,\nJiang Borui,\nDeng Chaorui,\nZhao Yang,\nLiu Dong,\nMu Yadong,\nTan Mingkui,\nWang Xinggang,\nLiu Wenyu,\nXiao Bin
This meta-analysis (148 studies, k = 197, N = 31,718) examined the relationship between motivation and transfer in professional training. For this purpose, motivation was conceptualized in the following nine dimensions: motivation to learn, motivation to transfer, pre- and post-training self-efficacy, mastery orientation, performance orientation, avoidance orientation, expectancy, and instrumentality. Population correlation estimates ranged between −0.11 and 0.52. Three moderator effects were estimated. First, correlations were higher when the training focused on declarative and self-regulatory, rather than on procedural, knowledge. Second, learner-centered environments tended to show greater numbers of positive correlations than did knowledge-centered environments. Third, when compared with external, supervisory, or peer assessment, self-assessment of transfer produced upwardly biased population estimates irrespective of the transfer criterion. These findings are discussed in terms of their implications for theories of training effectiveness and their significance for the practice of training evaluation.
引用情况为0引用
10.1016/j.neunet.2014.09.005
51.98
AAAI Conference on Artificial Intelligence (AAAI)（2021）
作者Jiawei Han获奖情况：2023 AI 2000 Most Influential Scholar Award Honorable Mention in Database\nsilver，2023 AI 2000 Most Influential Scholar Award Honorable Mention in Data Mining silver，2023 AI 2000 Most Influential Scholar Award Honorable Mention in Information Retrieval and Recommendation等\n作者Yu ZHANG获奖情况：2023 WWW 2023 Best Reviewer，2023 WSDM 2023 Student Travel Grant，2022 CIKM 2022 Best Reviewer等
论文引用数前三的三位作者各自的代表作如下：作者Bart Baesens引用量最高的论文为Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings；Wilfried Lemahieu引用量最高的论文为On the Definition of Service Granularity and Its Architectural Impact；Jochen De Weerdt引用量最高的论文为A multi-dimensional quality assessment of state-of-the-art process discovery algorithms using real-life event logs。
近两年Jan Vanthienen有发表论文Extracting Decision Dependencies and Decision Logic from Text Using Deep Learning Techniques，其他两位作者未有论文发表
Jen-Chuen Hsieh，Chang-Rung Chen，Jie-Li Tsai，Daisy L. Hung
Computational social scientist Mesysam Alizadeh has been harnessing the wealth of network and human social data available through social media platforms to understand the roots and spread of extremist ideology. In recent projects during his Ph.D. at George Mason University and postdoctoral fellowship at Indiana University Bloomington, Alizadeh has explored the moral and emotional factors underlying political extremism. He is also studying how extremism spreads on social media by analyzing the information sharing behavior of political extremists on Twitter. Meysam was a postdoctoral associate and associate research scholar at the Empirical Studies of Conflict Project at Princeton University, studying foreign influence efforts on democratic elections. In this project, advised by Professor Jacob Shapiro, Meysam used publicly available verified data sets of foreign online influence operations to train classifiers that can identify suspicious activities on social media.
Xiaodong Liu,\nPengcheng He,\nWeizhu Chen,\nJianfeng Gao
AAAI Conference on Artificial Intelligence (AAAI)（2021）
1.A systematic review of intelligent tutoring systems based on Gross body movement detected using computer vision 2.Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving 3.DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models\n4.1st Workshop on Maritime Computer Vision (MaCVi) 2023: Challenge Results 5.Exploring Data Augmentation Methods on Social Media Corpora
Ryan T. Mcdonald 主要研究方向用于机器翻译、问题回答、意见分析以及信息检索的核心算法和模型。Lev Arie Ratinov研究使用机器学习进行自然语言处理（NLP），尤其是信息提取，文本分类等。其重点是使用百科全书知识和半监督学习来减少注释工作并提供快速有效的解决方案， 科学界和行业。Mirella Lapata研究主要集中在从结构化和非结构化数据中表示、提取和生成语义信息的计算模型。Michael Collins主要研究方向是自然语言处理和机器学习。Daniel S. Weld主要研究方向为：人工智能、互联网系统、人机交互、自然语言处理。
Peter J. Haug、Guergana K. Savova、Brian E. Chapman、Michael M. Wagner、Marcelo Fiszman
杨柳，女。研究领域：机器学习、数据挖掘。研究方向：迁移学习、多视图学习、多标记学习；高宏，博士，教授，博士生导师，中国计算机学会数据库专委会委员、传感器网络专委会委员、学术工作委员会委员，2005年入选国家教育部新世纪优秀人才支持计划，同年获国家科技进步二等奖；陈端端，研究领域/方向 生物医学工程及医学仿真 生物力学 计算机辅助医疗；丁堃，研究方向：\n知识计量与知识发现、科学计量与科技管理、知识产权与创新管理、开放式创新与区域经济发展；陈韵岱，解放军老年心血管病研究所行政主任，研究方向：\n1．冠心病精准介入治疗\n2．心肌保护的临床及机制研究
\nLanguage Models are Few-Shot Learners\n\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\nLanguage Models are Unsupervised Multitask Learners\n\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5 B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.\n\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n\nMasked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.\n\nOn the Opportunities and Risks of Foundation Models\n\nAI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.
Philip S. Yu\nRoberto Romero陶大程(Dacheng Tao)沈定刚\n(Dinggang Shen)\nAziz Sheikh
李兰娟(Lanjuan Li)\n王宁利(Ningli Wang)\n韩德民(Demin Han)\n董家鸿(Jiahong Dong)\n杨敏(Min Yang)
2020年
computer science bibliography
2022年，级别未知
University of Bergen\nDepartment of Biological Sciences
University of Bern
Patient satisfaction with primary care: an observational study comparing anthroposophic and conventional care\nHealth status and health care utilisation of patients in complementary and conventional primary care in Switzerland—an observational study
I was previously at Microsoft Research and was part of a project aimed at using Machine Learning for first response during disaster situations. I have worked to make AI algorithms for speech more accessible by developing models for accented speech generation. More of my recent work has been on the theoretical side where I have been exploring Algorithmic notions of Randomness and Ergodic systems. I have also been looking at optimization techniques that use Langevin dynamics and other processes for convex and non convex functions. I also have been looking at graph sparsification using Spectral Graph theory.
以监督有限的机器学习为中心，专注于不确定性下的概率生成建模和顺序决策。
Computing Research Repository
斯坦福大学
Our findings thus demonstrate a close topographic correlation between cortical functional anatomy and direction-related information in humans that might be used for brain-machine interfacing.
Neurocomputing
有\nBPMN: An introduction to the standard
有\nAn Integrated Enterprise Modeling Framework Using the RUP/UML Business Use-Case Model and BPMN.\nUsing the RUP/UML Business Use Case Model for Service Development Governance: A Business and IT Alignment Based Approach